---
title: "Motor Failure Prediction – Unit 25"
author: "Noel Sebastia"
date: "2025.01.04"
output:
  html_notebook: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# 1. Fundamentos del Aprendizaje Automático

## 1.1 ¿Qué es el aprendizaje automático?

El aprendizaje automático (Machine Learning, ML) es una rama de la inteligencia artificial que permite a los sistemas aprender a partir de datos, identificar patrones y tomar decisiones sin estar explícitamente programados para cada tarea. A diferencia de los enfoques clásicos de programación, en los que un programador escribe reglas específicas, en ML se entrena un modelo con datos para que aprenda dichas reglas de forma automática.

En términos generales, un algoritmo de aprendizaje automático analiza conjuntos de datos históricos y desarrolla un modelo estadístico que puede generalizarse a nuevos datos. Esta capacidad de aprendizaje es lo que permite desarrollar sistemas adaptativos y predictivos.

Según Bishop (2006), “el objetivo del aprendizaje automático es desarrollar algoritmos que mejoren su rendimiento automáticamente a través de la experiencia”. Esto permite automatizar tareas complejas como la clasificación de imágenes, la predicción de fallos o el reconocimiento de voz.

> Referencia: Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.


## 1.2 Tipos de aprendizaje automático

El aprendizaje automático puede clasificarse en tres tipos principales:

- **Aprendizaje supervisado:** El modelo se entrena con datos etiquetados (input y output). Ejemplos: regresión lineal, árboles de decisión, SVM, redes neuronales. Se utiliza, por ejemplo, para predecir si un motor fallará basándose en lecturas de sensores.

- **Aprendizaje no supervisado:** El modelo trabaja con datos no etiquetados para encontrar estructuras o patrones ocultos. Ejemplos: clustering (K-means), reducción de dimensionalidad (PCA).

- **Aprendizaje por refuerzo:** El sistema aprende mediante prueba y error, maximizando una recompensa a largo plazo. Se aplica comúnmente en robótica, juegos o control industrial.

En este trabajo, el enfoque será el aprendizaje **supervisado**, ya que se dispone de datos etiquetados indicando si un motor ha fallado o no.

## 1.3 ¿Qué es una máquina inteligente?

Una máquina inteligente es un sistema capaz de percibir su entorno, analizar datos, aprender de la experiencia y tomar decisiones de forma autónoma o asistida. Su comportamiento no depende únicamente de reglas programadas previamente, sino que evoluciona con la información que recibe.

Estas máquinas combinan sensores, algoritmos de procesamiento, capacidad de almacenamiento y mecanismos de decisión adaptativos. En el contexto de la industria, esto puede abarcar desde un sistema de climatización que ajusta su rendimiento según patrones de uso, hasta motores que se auto-diagnostican y predicen fallos futuros.

Las máquinas inteligentes representan un paso más allá de la automatización tradicional, ya que integran inteligencia en la operación y no sólo ejecución de instrucciones.

## 1.4 ¿Por qué el aprendizaje automático es esencial para diseñarlas?

El aprendizaje automático es la base del comportamiento inteligente en sistemas modernos. Sin él, las máquinas están limitadas a lo que el programador haya previsto. Con ML, en cambio, los sistemas pueden:

- **Aprender y adaptarse**: Cambian su comportamiento en función de nuevos datos.
- **Detectar patrones complejos**: Por ejemplo, vibraciones anómalas que preceden a una avería.
- **Reducir la intervención humana**: Automatizan tareas de diagnóstico o predicción.
- **Mejorar con el tiempo**: Aumentan su precisión cuanto más datos procesan.

En el campo del mantenimiento industrial, el ML es esencial para la transición del mantenimiento reactivo al **mantenimiento predictivo**, en el cual los sistemas detectan condiciones anómalas antes de que ocurran fallos graves.

Como indica Russell y Norvig (2020), el aprendizaje automático transforma datos en conocimiento operativo, lo cual es indispensable para la inteligencia de sistemas complejos.

> Referencia: Russell, S. J. & Norvig, P. (2020). *Artificial Intelligence: A Modern Approach* (4th ed.). Pearson.

## 1.5 Aplicación al caso: Predicción de fallo en motores

En este trabajo se aplicará el aprendizaje automático a un problema industrial real: predecir el fallo de motores eléctricos mediante datos captados por sensores. Estos motores son elementos clave en muchas líneas de producción, y su parada inesperada genera importantes costes económicos y logísticos.

Mediante el uso de modelos de ML y datos como temperatura, vibración, humedad o carga, es posible entrenar un sistema que anticipe cuándo un motor está cerca de fallar. Esto convierte al sistema en una máquina inteligente capaz de:

- Evaluar su estado interno en tiempo real.
- Alertar al operario antes del fallo.
- Posiblemente, indicar la **causa más probable del fallo** si se extiende el modelo.

El uso de estos sistemas reduce tiempos de inactividad, mejora la planificación del mantenimiento y aumenta la seguridad operativa. Además, se puede ampliar el enfoque a distintas clases de fallo (eléctrico, mecánico, por sobreuso), lo que permitiría diagnósticos más específicos.

Este tipo de solución ilustra perfectamente cómo el aprendizaje automático contribuye al diseño de máquinas inteligentes dentro del entorno industrial.

> Referencia: Ghosh, S. et al. (2021). *Predictive Maintenance using Machine Learning*. Springer.


# 2. Presentación y origen del dataset

## 2.1 Descripción del dataset

El conjunto de datos utilizado en este trabajo proviene de un entorno industrial simulado diseñado para el estudio del mantenimiento predictivo en sistemas rotativos. Fue obtenido del repositorio oficial UCI Machine Learning Repository (AI4I 2020 Predictive Maintenance Dataset) y contiene un total de 1.000 registros y 10 variables que representan mediciones relevantes para diagnosticar el estado de funcionamiento de un sistema rotativo o motor.

Las variables incluidas en el dataset son:

- `UDI`: Identificador único del registro (numérico).
- `Product ID`: Código del producto o motor (carácter).
- `Type`: Tipo de motor, clasificado como `H` (High), `L` (Low) o `M` (Medium).
- `Air temperature [K]`: Temperatura del aire medida en Kelvin.
- `Process temperature [K]`: Temperatura del proceso industrial (Kelvin).
- `Rotational speed [rpm]`: Velocidad de rotación en revoluciones por minuto.
- `Torque [Nm]`: Par de torsión aplicado al motor (Newton-metros).
- `Tool wear [min]`: Desgaste de la herramienta en minutos acumulados.
- `Target`: Variable binaria que indica si hubo un fallo (1) o no (0).
- `Failure Type`: Tipo específico de fallo registrado, como por ejemplo "Power Failure" o "Tool Wear Failure".

El dataset está completamente limpio, sin valores faltantes en ninguna de las columnas. Las clases de fallo están desbalanceadas: un 70% de las muestras corresponden a motores sin fallo (`Target = 0`) y el 30% presentan algún tipo de fallo (`Target = 1`). Entre las categorías de fallo más frecuentes se encuentran "Tool Wear Failure" (119 casos) y "Heat Dissipation Failure" (43 casos).

En cuanto a los sensores, las variables numéricas como la temperatura, la velocidad rotacional o el torque presentan rangos coherentes con un entorno industrial, con valores como:

- `Air temperature [K]`: Media de 300.1K (~27°C), rango entre 294.2K y 305.3K.
- `Rotational speed [rpm]`: Media de 1500 rpm, valores entre 1201 y 1893 rpm.
- `Torque [Nm]`: Media de 39.4 Nm, variando desde 9.9 hasta 72.4 Nm.

Este dataset es adecuado para construir modelos supervisados de clasificación que permitan predecir la ocurrencia de fallos basándose en los datos capturados por los sensores.

## 2.2 Objetivo del análisis

El objetivo principal de este proyecto es construir un modelo de aprendizaje automático que permita predecir si un motor eléctrico fallará en función de los datos sensoriales registrados durante su operación. Esto se traduce en un problema de **clasificación binaria supervisada**, donde la variable objetivo (`Target`) toma los valores 0 (sin fallo) o 1 (con fallo).

Este tipo de predicción es crucial en contextos industriales porque permite anticiparse a posibles interrupciones de producción, programar mantenimientos preventivos y evitar fallos catastróficos que afecten la seguridad o la eficiencia operativa.

Como extensión futura, se plantea la posibilidad de realizar una **clasificación multiclase** que no sólo indique la ocurrencia de un fallo, sino que también determine su **tipo probable** (por ejemplo: fallo por sobreesfuerzo, fallo eléctrico, desgaste de herramienta, etc.), utilizando la columna `Failure Type` como variable objetivo.

---

# 3. Análisis exploratorio de datos (EDA)

Este apartado tiene como objetivo comprender el comportamiento de las variables del dataset y justificar cada decisión tomada antes de construir el modelo predictivo. Cada gráfico se incluye con una intención específica: evaluar la calidad de los datos, detectar patrones relevantes para la predicción o anticipar decisiones de preprocesado.

```{r load-data}
# Cargar librerías necesarias
library(tidyverse)
library(ggplot2)
library(readr)
library(DT)

df <- read_csv("data/predictive_maintenance.csv")

head(df)
```

La tabla anterior muestra las primeras filas del dataset, lo que permite una primera inspección visual de su estructura. Se observan variables numéricas como Rotational speed [rpm], Torque [Nm] o Tool wear [min], así como variables categóricas (Type) e identificadores (UDI, Product ID). Esta revisión inicial permite identificar rápidamente el tipo de datos presentes, su formato y si contienen valores anómalos evidentes.

## 3.1 Estructura y limpieza de datos

Antes de aplicar cualquier técnica de análisis o modelado, es fundamental **verificar la integridad del dataset**, especialmente la presencia de valores faltantes. Esto permite determinar si es necesario aplicar métodos de imputación, eliminar registros incompletos o si se puede continuar con el conjunto de datos tal como está.

```{r load-packages}
colSums(is.na(df))
```

La salida de este comando indica que **no existen valores NA en ninguna de las columnas**. Por tanto, se confirma que el dataset está completamente limpio en cuanto a datos ausentes, lo cual simplifica el proceso de preprocesado posterior y evita posibles sesgos derivados de imputaciones artificiales.

## 3.2 Distribución de la variable objetivo (Target)

Antes de aplicar cualquier modelo de clasificación, es importante entender cómo se distribuyen las clases en la variable objetivo. En este caso, `Target` indica si un motor ha fallado (`1`) o no (`0`). Analizar su distribución permite anticipar si será necesario aplicar técnicas de balanceo de clases.

```{r Target-distribution}
ggplot(df, aes(x = factor(Target))) +
  geom_bar(fill = "steelblue") +
  labs(title = "Distribución de la variable objetivo (Target)", x = "Fallo", y = "Cantidad")
```

El gráfico de barras muestra que aproximadamente el **70% de los motores no presentan fallos**, mientras que el **30% sí fallan**. Este **desbalance moderado** es relevante, ya que puede afectar el rendimiento de los modelos de clasificación, haciéndolos propensos a favorecer la clase mayoritaria.

Por tanto, este análisis justifica el uso posterior de técnicas como el **sobremuestreo (upsampling)** para equilibrar la representación de clases durante el entrenamiento y mejorar la capacidad del modelo para detectar fallos.

## 3.3 Distribución de variables numéricas relevantes

En este apartado se analizan algunas de las variables numéricas más representativas del estado operativo del motor. El objetivo es comprender su distribución, identificar posibles valores atípicos y evaluar su comportamiento antes del modelado. Las variables seleccionadas han sido escogidas por su **relación directa con el rendimiento o desgaste del motor**, según la documentación del dataset.

### Temperatura del aire (`Air temperature [K]`)

La temperatura ambiente afecta directamente a la eficiencia térmica del motor. Se analiza su distribución para detectar sesgos o valores extremos que puedan requerir transformación o tratamiento adicional.

En esta sección se representa la temperatura en su forma original, Kelvin, tal como aparece en el dataset bruto. Aunque esta unidad es válida físicamente, en la etapa de preprocesamiento (Sección 4) se convertirá a grados Celsius.

```{r air-temperature}
ggplot(df, aes(x = `Air temperature [K]`)) +
  geom_histogram(bins = 30, fill = "tomato", color = "black") +
  labs(title = "Temperatura del aire (K)", x = "Kelvin", y = "Frecuencia")
```

La variable presenta una distribución **ligeramente sesgada**, pero **aproximadamente normal**, lo cual sugiere que puede utilizarse directamente en los modelos sin necesidad de transformación. No se observan valores atípicos evidentes.

En el preprocesado posterior, esta variable será transformada a grados Celsius para facilitar su interpretación.

### Torque (`Torque [Nm]`)

El torque mide la fuerza de rotación aplicada, siendo un parámetro clave en condiciones de carga. Se visualiza para identificar su comportamiento general y evaluar su relación potencial con el fallo del motor.

```{r torque}
ggplot(df, aes(x = `Torque [Nm]`)) +
  geom_histogram(bins = 30, fill = "darkgreen", color = "black") +
  labs(title = "Distribución del Torque", x = "Nm", y = "Frecuencia")
```

El torque presenta una **distribución ligeramente sesgada hacia la izquierda**, con un único pico principal alrededor de 40 Nm. No se observan modos secundarios que sugieran regímenes de operación diferenciados, pero la forma de la distribución podría estar relacionada con distintas cargas aplicadas durante el uso. Este comportamiento justifica su inclusión como predictor clave en el modelo.

### Velocidad rotacional (`Rotational speed [rpm]`) por tipo de motor (`Type`)

En lugar de graficar la distribución general, se ha optado por un **boxplot por tipo de motor**, ya que la variable `Type` segmenta los motores según su capacidad o configuración. Esto permite comparar directamente si existen diferencias operativas entre tipos.

```{r rotational-speed}
ggplot(df, aes(x = Type, y = `Rotational speed [rpm]`, fill = Type)) +
  geom_boxplot() +
  labs(title = "Velocidad rotacional por tipo de motor", x = "Tipo", y = "RPM")
```

El gráfico de boxplot permite comparar la distribución de la velocidad rotacional (`RPM`) entre los distintos tipos de motor (`Type`). Aunque **las medianas y rangos centrales son bastante similares** para los tres grupos (`L`, `M`, `H`), se observa la presencia de **algunos valores atípicos**, especialmente en los tipos `H` y `M`.

Estos valores extremos podrían reflejar **condiciones operativas poco frecuentes** o situaciones de carga elevada, lo que **refuerza la idea de que tanto** `Type` **como** `Rotational speed [rpm]` **podrían ser variables predictoras importantes** en la detección de fallos.

## 3.4 Correlaciones entre variables

Antes de entrenar modelos de tipo lineal, como la regresión logística, es importante analizar las correlaciones entre variables numéricas. El objetivo es **detectar relaciones lineales fuertes que puedan generar multicolinealidad**, es decir, redundancia estadística entre predictores. Este fenómeno puede afectar tanto la interpretación del modelo como su estabilidad y rendimiento.

En este análisis se incluyen únicamente variables numéricas relevantes desde el punto de vista operativo. Se han excluido variables duplicadas en diferentes unidades (como las temperaturas en Kelvin y Celsius) y campos identificadores como `UDI`.

```{r correlations}
library(corrplot)
numeric_vars <- df %>%
  select(`Rotational speed [rpm]`, `Torque [Nm]`, `Tool wear [min]`,
         `Air temperature [K]`, Target)

cor_matrix <- cor(numeric_vars)
corrplot(cor_matrix, method = "color", type = "upper",
         col = colorRampPalette(c("blue", "white", "red"))(200),
         tl.col = "black", addCoef.col = "black")
```

El gráfico obtenido muestra que **no existe una correlación lineal fuerte entre las variables analizadas**. De hecho, la mayoría de coeficientes de correlación están muy próximos a cero, indicando independencia lineal.

Algunas observaciones específicas:

- La mayor correlación observada es entre `Air temperature [C]` y `Torque [Nm]`, pero apenas alcanza un valor de **-0.06**, lo que no es estadísticamente significativo.

- Otras relaciones como entre `Torque` y `Tool wear`, o `Rotational speed` y `Target`, también muestran coeficientes **cercanos a cero**.

- Esto indica que **no hay redundancia estadística evidente** entre los predictores, al menos desde el punto de vista de la correlación lineal.

Este resultado sugiere que las variables pueden aportar información complementaria al modelo, lo que es positivo para la construcción de predictores independientes. La multicolinealidad será, no obstante, evaluada más adelante mediante el análisis de VIF tras el preprocesamiento.

---

# 4. Preparación de los datos

Antes de entrenar cualquier modelo de aprendizaje automático, es imprescindible preparar adecuadamente los datos. Esta fase tiene un impacto directo en el rendimiento y la interpretabilidad del modelo. En este caso, se han llevado a cabo las siguientes acciones:

- **Conversión de unidades**: Las temperaturas originalmente estaban expresadas en Kelvin. Aunque es una unidad válida desde el punto de vista físico, se decidió convertirlas a grados Celsius para facilitar su análisis e interpretación por parte de técnicos y responsables industriales.

- **Eliminación de columnas irrelevantes**: Se eliminaron `UDI` y `Product ID` por ser identificadores sin información predictiva. También se eliminaron `Air temperature [K]` y `Process temperature [K]` tras calcular sus equivalentes en °C, evitando así redundancias que podrían interferir con el aprendizaje del modelo.

- **Tratamiento de variables categóricas**: La variable `Type`, que indica el tipo de motor (H, M, L), fue inicialmente codificada como dummies (*one-hot encoding*). Sin embargo, tras una revisión metodológica, se decidió tratarla como un factor. Esto permite a ciertos algoritmos (*como la regresión logística en R*) manejarla de forma más eficiente y evita introducir colinealidad innecesaria entre columnas.

- **Balanceo de clases**: Dado que solo el 30% de los motores presentan fallo (Target = 1), se aplicó sobremuestreo para igualar ambas clases en el conjunto de entrenamiento. Esta estrategia mejora la sensibilidad del modelo hacia la clase minoritaria y reduce el riesgo de un sesgo hacia la clase mayoritaria.

- **Verificación de multicolinealidad**: Como paso final del preprocesado, se ha evaluado la independencia estadística entre predictores mediante el análisis del **Factor de Inflación de Varianza** (VIF). Esto es especialmente relevante antes de entrenar modelos lineales como la regresión logística

El preprocesado de datos incluye una etapa crítica cuando se trabaja con conjuntos desbalanceados: el *up-sampling*. Esta técnica consiste en aumentar artificialmente el número de muestras de la clase minoritaria (en nuestro caso, los motores con fallo), duplicando aleatoriamente ejemplos de esa clase hasta igualar el número de la clase mayoritaria.

El objetivo del *up-sampling* es evitar que el modelo aprenda a ignorar la clase minoritaria por su baja representación. Esto mejora métricas como la sensibilidad (recall) y el F1-score en problemas donde detectar la clase minoritaria (fallos) es crítico. Sin embargo, también incrementa el riesgo de *overfitting* si se aplicara en exceso, ya que se repiten datos existentes en lugar de aportar ejemplos nuevos.

En este proyecto, se ha utilizado la función `upSample()` del paquete `caret` para igualar el número de casos con y sin fallo antes de entrenar el modelo.

## 4.1 Conversión de unidades

Como parte del preprocesamiento, se convierte la variable Air temperature [K] a grados Celsius (Air temperature [C]) para facilitar su interpretación. La versión en Kelvin será eliminada en el siguiente paso para evitar redundancias.

```{r convert-units}
df$`Air temperature [C]` <- df$`Air temperature [K]` - 273.15

ggplot(df, aes(x = `Air temperature [C]`)) +
  geom_histogram(bins = 30, fill = "tomato", color = "black") +
  labs(title = "Distribución de la temperatura del aire (°C)",
       x = "°C", y = "Frecuencia")
```

Esta nueva visualización confirma que, tras la conversión, la **distribución de la temperatura en grados Celsius mantiene su forma aproximadamente normal**, sin presencia de valores extremos. Esta transformación no altera el comportamiento estadístico de la variable, pero **sí mejora su interpretabilidad**.

## 4.2 Eliminación de variables redundantes

```{r remove-redundant}
df <- df %>% select(-UDI, -`Product ID`, -`Air temperature [K]`, -`Process temperature [K]`)

head(df)
```

`UDI` y `Product ID`: identificadores únicos, sin correlación con la variable objetivo ni uso práctico para predicción.

`Air temperature [K]` y `Process temperature [K]`: sus equivalentes en °C ya fueron generados previamente para facilitar análisis más intuitivos, por lo que mantener ambas versiones genera redundancia y puede afectar a los modelos.

Tras este paso, el conjunto de datos ya no incluye columnas irrelevantes como `UDI`, `Product ID`, ni versiones duplicadas de temperatura en Kelvin. Esto puede verificarse observando el contenido actual del dataset (`head(df)`), donde solo permanecen las variables con valor predictivo real y sin redundancias.

## 4.3 Conversión de variables categóricas

```{r convert-categorical}
df$Type <- as.factor(df$Type)
df$Target <- as.factor(df$Target)

head(df)
```

Aunque `Target` ya contiene valores 0 y 1, se convierte en factor para asegurar que se trata como una variable categórica en los modelos de clasificación. Esto también evita que ciertas funciones interpreten erróneamente la variable como continua. La conversión de `Type` permite que modelos como `glm()` o árboles de decisión puedan interpretar directamente la categoría sin codificación adicional.

Como resultado de esta transformación, las variables `Type` y `Target` ahora aparecen como factores (`<fctr>`), lo que indica que han pasado a tratarse correctamente como categorías en el análisis. Esto puede verse en la cabecera del dataset, donde antes `Type` era una cadena de caracteres (`<chr>`) y ahora se interpreta como factor, permitiendo un tratamiento adecuado en modelos de clasificación.

### 4.4 Balanceo de clases

A partir del análisis EDA (Sección 3.2) observamos que la clase *Target = 1* representa sólo ≈30% de los registros. Para que el modelo aprenda a detectar fallos sin sesgo hacia la clase mayoritaria es necesario equilibrar las clases **dentro del conjunto de entrenamiento**.  
A continuación se describen y comparan tres estrategias clásicas de balanceo.

---

#### 4.4.1 Sobremuestreo aleatorio (Upsampling)

**¿Qué es el sobremuestreo aleatorio y por qué lo utilizamos?**  
El *upsampling* (sobremuestreo aleatorio) consiste en **duplicar aleatoriamente** registros de la clase minoritaria (*Target=1*) hasta igualar su tamaño al de la clase mayoritaria. De este modo, el algoritmo recibe un conjunto de entrenamiento balanceado y evita aprender un sesgo sistemático contra los fallos, mejorando la sensibilidad (*recall*).

*Principio clave*: no se generan datos sintéticos —cada fila que se replica proviene de un caso real—, lo que preserva la trazabilidad del dataset y facilita la justificación ante auditores industriales.

*Cuándo es útil*: cuando el coste de un falso negativo (no detectar un fallo) es mucho mayor que el de un falso positivo y disponemos de un número moderado de observaciones.

*Limitación*: al repetir información, puede aumentar el riesgo de *overfitting* si el modelo memoriza los duplicados en lugar de aprender patrones generales.

A continuación se aplica la función `upSample()` (paquete **caret**) para balancear el **data frame original** `df`, produciendo un nuevo objeto `df_balanced` que se usará en los experimentos posteriores:


```{r 4-4-1-upsampling, echo=TRUE}
library(caret)
set.seed(123)
#duplica aleatoriamente registros de la clase minoritaria (Target=1)
df_balanced <- upSample(x = df[, -which(names(df) == "Target")],
                        y = df$Target,
                        yname = "Target")

# Comprobar nuevo balance
table(df_balanced$Target)
```
Como se puede observar, el nuevo conjunto de datos `df_balanced` tiene un número **idéntico de registros** para ambas clases (Target=0 y Target=1). Esto asegura que el modelo no esté sesgado hacia la clase mayoritaria durante el entrenamiento.

---

#### 4.4.2 Comparación de técnicas alternativas

##### Objetivo y motivación  
Reforzar la decisión metodológica demostrando que el upsampling simple es —o no— la mejor opción frente a:

* **SMOTE** – genera ejemplos sintéticos interpolando vecinos de la clase minoritaria.
* **Undersampling** – elimina registros aleatorios de la clase mayoritaria.

Se mantienen el mismo *split* 80/20, la misma semilla y el mismo modelo base (regresión logística).  Evaluamos **AUC**, **Balanced Accuracy** y **F1‑score** sobre el conjunto de *test* correspondiente a cada técnica.

```{r 4-4-2-setup, message=FALSE, warning=FALSE}

library(DMwR2)   # SMOTE original
library(ROSE)    # undersampling
library(pROC)
library(yardstick)
library(caret)
set.seed(123)

# función de métrica común
eval_metrics <- function(df_resampled, label){
  idx <- createDataPartition(df_resampled$Target, p = 0.8, list = FALSE)
  train <- df_resampled[idx, ]
  test  <- df_resampled[-idx, ]
  model <- glm(Target ~ ., data = train, family = binomial())
  prob  <- predict(model, newdata = test, type = "response")
  pred  <- factor(ifelse(prob > 0.5, 1, 0), levels = levels(test$Target))
  tibble(Estrategia = label,
         AUC        = roc(test$Target, prob, levels = rev(levels(test$Target)))$auc |> as.numeric(),
         BalAcc     = bal_accuracy_vec(test$Target, pred),
         F1         = f_meas_vec      (test$Target, pred, event_level = "second"))
}
```

##### **Implementación**
```{r 4-4-2-smote}
#SMOTE - se genera dataframe completo para mantener paralelismo con df_balanced
df_smote <- smote_func(Target ~ ., data = df, perc.over = 200, perc.under = 150)
res_smote <- eval_metrics(df_smote, "SMOTE")
```

```{r 4-4-2-under}
# Undersampling – random downSample (caret)
# Selecciona una muestra de la clase mayoritaria del mismo tamaño que la minoritaria

df_under <- downSample(x = df[, -which(names(df) == "Target")],
                       y = df$Target,
                       yname = "Target")
res_under <- eval_metrics(df_under, "Undersampling")
```

La métrica de referencia (upsampling) se calcula una sola vez para evitar repetir trabajo:
```{r 4-4-2-ref}
res_up <- eval_metrics(df_balanced, "Upsampling")
resultados <- bind_rows(res_up, res_smote, res_under) |> 
              mutate(across(where(is.numeric), round, 3))
knitr::kable(resultados, caption = "Comparativa de técnicas de balanceo sobre su respectivo conjunto de test")
```

##### **Resultados y discusión**

La ejecución sobre este dataset arrojó **métricas perfectas en las tres técnicas**.

Esto evidencia que el problema presenta una **separación perfecta** con las variables actuales: la regresión logística encuentra una frontera que clasifica sin error, de modo que el balanceo de clases no altera la capacidad predictiva.

**¿Por qué sucede?**  

• Los predictores capturan información extremadamente discriminante (p.ej. combinaciones de torque+velocidad).  
• El dataset es sintético y muy limpio; entrenamiento y prueba provienen del mismo dominio.  
• Una frontera lineal basta para separar las clases; cualquier re‑muestra conserva esa linealidad.

Aunque las métricas son idénticas, la **complejidad del conjunto de entrenamiento** sí varía:

* **Upsampling** → ~1400 filas (duplica la clase minoritaria).
* **SMOTE** → ~1400 filas con registros **sintéticos**.
* **Undersampling** → ~600 filas (descarta ~65% de la clase mayoritaria).

Desde la óptica de costes computacionales y trazabilidad, resulta más práctico mantener el upsampling que generar ejemplos sintéticos o perder observaciones reales.

##### **Conclusión operativa**

Dado que las tres estrategias producen un rendimiento idéntico, **nos quedamos con el upsampling aleatorio** por ser:

* sencillo de implementar y auditable;
* libre de datos sintéticos difíciles de justificar ante un auditor;
* coherente con el pipeline existente sin inflar excesivamente el tamaño del dataset.

Dado que el upsampling aleatorio presenta la mejor combinación de métricas **sin introducir datos artificiales** (caso SMOTE) ni descartar observaciones reales (caso undersampling), se confirma como la técnica de balanceo definitiva del pipeline.


## 4.5 Verificación de multicolinealidad tras el preprocesado

Una vez creado el conjunto balanceado de datos (`df_balanced`), se ha procedido a comprobar la **multicolinealidad entre predictores** mediante el cálculo del **Variance Inflation Factor (VIF)**. Este análisis es clave antes de entrenar modelos lineales, ya que la presencia de colinealidad puede afectar negativamente tanto al ajuste como a la interpretación de los coeficientes del modelo.

El análisis se ha realizado utilizando el conjunto balanceado completo, justo antes de dividir en entrenamiento y test. En el caso de variables categóricas como `Type` y `Failure Type`, el modelo calcula el **Generalized VIF (GVIF)**, que se ajusta en función del número de niveles del factor. Para interpretar correctamente estos valores, se utiliza la transformación estándar `GVIF^(1/(2*Df))`, que permite comparar todos los predictores en la misma escala.


```{r vif-after-preprocessing}
library(car)
model_vif <- glm(Target ~ ., data = df_balanced, family = binomial())
vif_values <- vif(model_vif)
print(vif_values)
```

Resumen de los valores ajustados de VIF (GVIF^(1/(2*Df))):

- **Type**: 1.0107
- **Rotational speed [rpm]**: 1.0144
- **Torque [Nm]**: 1.0129
- **Tool wear [min]**: 1.0083
- **Failure Type**: 1.0084
- **Air temperature [C]**: 1.0132

Todos los valores están muy próximos a 1, lo que indica una colinealidad prácticamente inexistente entre los predictores del modelo. Esto es especialmente positivo, ya que refuerza la independencia de las variables seleccionadas y evita distorsiones en la estimación de los coeficientes.

Ninguno de los predictores supera el umbral habitual de 5 (considerado como colinealidad moderada), ni mucho menos el valor de 10 (colinealidad severa). Por tanto, no es necesario eliminar ni transformar ninguna variable adicional en esta etapa.

---

# 5. Modelado

## 5.1 Algoritmo(s) utilizados

Para esta primera aproximación, se ha optado por utilizar un modelo de **regresión logística**, una técnica clásica pero potente para problemas de **clasificación binaria**. Su funcionamiento se basa en estimar la probabilidad de pertenencia a una clase en función de una combinación lineal de las variables predictoras, transformada mediante una función logística (sigmoide).

Este modelo es especialmente útil por varias razones:

- **Interpretabilidad**: permite identificar fácilmente qué variables tienen mayor peso en la predicción del fallo, lo cual es valioso en contextos industriales donde se requiere justificar las decisiones.

- **Velocidad y simplicidad**: es rápido de entrenar y no requiere grandes cantidades de datos ni un ajuste excesivo de hiperparámetros.

- **Robustez**: cuando los predictores están correctamente preparados y no presentan multicolinealidad, como es el caso tras el análisis con VIF, el modelo suele ofrecer buenos resultados iniciales.

Aunque la regresión logística es un modelo lineal, ofrece una línea base de rendimiento razonable con la que se pueden comparar otros algoritmos más complejos. En secciones posteriores se evaluará también el comportamiento de un modelo Naïve Bayes, con el objetivo de explorar posibles diferencias en rendimiento y sensibilidad frente al tamaño de muestra.

## 5.2 División train/test

Con el objetivo de evaluar la capacidad de generalización del modelo, se ha dividido el conjunto de datos balanceado (`df_balanced`) en dos subconjuntos:

**80% para entrenamiento**, donde el modelo aprende los patrones a partir de los datos.

**20% para test**, que se reserva para una evaluación objetiva del rendimiento sobre datos no vistos durante el entrenamiento.

Esta división se realiza utilizando la función `createDataPartition()` del paquete `caret`, que además de dividir aleatoriamente, estratifica automáticamente la variable objetivo (`Target`). Esto asegura que ambas clases (fallo/no fallo) se mantengan en proporciones similares tanto en el conjunto de entrenamiento como en el de test, lo cual es especialmente relevante en problemas de clasificación binaria.

```{r split-data}
# Cargar librerías necesarias
library(caret)
library(e1071)
set.seed(123)

# División en entrenamiento y test
trainIndex <- createDataPartition(df_balanced$Target, p = 0.8, list = FALSE)
trainData <- df_balanced[trainIndex, ]
testData  <- df_balanced[-trainIndex, ]
```

Esta separación permite una evaluación más fiable del rendimiento del modelo, minimizando el riesgo de sobreajuste (overfitting) al asegurar que las predicciones se validan con observaciones independientes del proceso de entrenamiento.

## 5.3 Entrenamiento del modelo

El modelo de **regresión logística** se ha entrenado utilizando el conjunto de entrenamiento definido en el paso anterior. Esta técnica permite modelar la probabilidad de que un motor falle (`Target = 1`) en función de las variables sensoriales disponibles, como torque, velocidad de rotación o temperatura.

El entrenamiento se ha llevado a cabo con la función `train()` del paquete `caret`, que proporciona una interfaz uniforme para entrenar modelos y realizar validación cruzada de forma automatizada. En este caso, se ha especificado el método `"glm"` (generalized linear model) con familia `"binomial"` para resolver un problema de clasificación binaria.

```{r model-training}
# Entrenamiento con regresión logística
model_log <- train(Target ~ ., data = trainData, method = "glm", family = "binomial")
```

Este proceso ajusta los coeficientes del modelo para minimizar la devianza residual, una medida que indica la discrepancia entre los valores observados y los predichos por el modelo. El resultado es una función logística que estima la probabilidad de fallo para cada observación en base a sus características.

## 5.4 Evaluación del modelo

Una vez entrenado el modelo de regresión logística, se ha evaluado su rendimiento utilizando el conjunto de test. Para ello, se generan predicciones sobre datos no vistos durante el entrenamiento y se comparan con las etiquetas reales. El análisis se realiza mediante una matriz de confusión, a partir de la cual se calculan métricas fundamentales en clasificación binaria:

- **Accuracy**: proporción de predicciones correctas sobre el total.

- **Sensitivity (Recall)**: capacidad del modelo para detectar correctamente los casos positivos (fallos).

- **Specificity**: capacidad de identificar correctamente los casos negativos (sin fallo).

- **Precision**: proporción de verdaderos positivos entre los predichos como positivos.

- **F1-score**: media armónica entre precisión y recall, útil especialmente con clases desbalanceadas.

- **Kappa**: coeficiente que mide la concordancia entre predicción y realidad, ajustado por azar.

```{r model-evaluation}
# Predicción
predictions <- predict(model_log, newdata = testData)

# Matriz de confusión y métricas
confusionMatrix(predictions, testData$Target)
```

En este caso, el modelo alcanzó un rendimiento **perfecto** sobre el conjunto de test, con los siguientes resultados:

- **Accuracy**: 1.0 (100% de aciertos)
- **Sensitivity (Recall)**: 1.0
- **Specificity**: 1.0
- **Pos Pred Value (Precision)**: 1.0
- **Negative Predictive Value**: 1.00
- **Balanced Accuracy**: 1.0
- **Kappa**: 1.0
- **No Information Rate (NIR)**: 0.5
- **P-Value [Acc > NIR]**: < 2.2e-16 (indica que el modelo es significativamente mejor que una predicción aleatoria)

La matriz de confusión muestra 140 predicciones correctas para cada clase y ningún error de clasificación.

Aunque este rendimiento puede parecer ideal, debe interpretarse con precaución:

- El conjunto fue balanceado artificialmente mediante upsampling, lo que facilita el aprendizaje del modelo.

- Los datos de test provienen del mismo origen que los de entrenamiento, lo que reduce la heterogeneidad.

- La validación cruzada y la evaluación sobre un conjunto no balanceado serían pasos importantes para confirmar la robustez del modelo.

### 5.4.1 Curva ROC y calibración

El proposito de este subapartado es complementar la matriz de confusión con dos visuales estándar en clasificación binaria:

**Curva ROC** para medir la capacidad global de discriminación mediante el área bajo la curva (AUC).

**Curva de calibración** para comprobar que las probabilidades estimadas se corresponden con la frecuencia observada de fallos.

Esta sección se coloca antes del antiguo apartado 5.5 (ahora 5.6 Evaluación del riesgo de *overfitting*) para mantener la lógica: primero evaluación numérica, después visual, y por último análisis de robustez.

---

#### 5.4.1 Curva ROC

```{r roc-curve, echo=TRUE}
# Se asume que `model_log` está entrenado y `testData` contiene las columnas originales
# caret::predict.train → type = "prob" devuelve df de probabilidades
probabilities <- predict(model_log, newdata = testData, type = "prob")[, "1"]
roc_obj <- pROC::roc(response = testData$Target,
                     predictor = probabilities,
                     levels   = rev(levels(testData$Target)))
plot(roc_obj, legacy.axes = TRUE, print.thres = TRUE,
     main = "Curva ROC – Regresión logística")
text(0.6, 0.2, labels = paste("AUC =", round(pROC::auc(roc_obj), 3)))
```

*Interpretación rápida*: un AUC = 1 indica separación perfecta; valores ≥0.9 se consideran excelentes.

---

#### 5.5.2 Curva de calibración

```{r calibration-curve, echo=TRUE}
# Construimos deciles de probabilidad
calib_tbl <- testData %>%
  mutate(prob = probabilities) %>%
  mutate(bin = ntile(prob, 10)) %>%
  group_by(bin) %>%
  summarise(prob_pred = mean(prob),
            prob_obs  = mean(as.numeric(as.character(Target))))

ggplot(calib_tbl, aes(x = prob_pred, y = prob_obs)) +
  geom_point(size = 3) +
  geom_abline(lty = 2) +
  coord_equal() +
  labs(title = "Curva de calibración (deciles)",
       x = "Probabilidad predicha",
       y = "Frecuencia observada")
```

*Cómo leerla*: la diagonal punteada representa la calibración ideal. Si los puntos caen sobre esa línea, el modelo asigna probabilidades realistas; si se desvían, necesita recalibración (p.ej. con *Platt scaling* o *isotonic regression*).

---

#### 5.5.3 Discusión

**Discriminación** · La curva ROC queda pegada al eje superior izquierdo y el AUC vale 1→el modelo separa las clases sin error.

**Calibración** · Obsérvese que la curva sólo muestra **dos puntos** (0% y 100%) porque el modelo produce probabilidades extremas (~0 o ~1) y, en consecuencia, los deciles se agrupan en los extremos. La alineación exacta sobre la diagonal significa que cuando predice 1 mantiene un 100% de aciertos y cuando predice 0 no hay falsos negativos; es coherente con la matriz de confusión perfecta.

**Implicación práctica** · Un modelo que arroja probabilidades tan binarias es útil como detector categórico, pero no discrimina “grados de riesgo”. Si en producción se desea priorizar mantenimientos en función de la probabilidad, habrá que recalibrar sobre datos reales (no balanceados) o emplear un clasificador que genere salidas menos extremas.

**Límite** · El *test* sigue siendo parte del dataset simulado y balanceado; en un entorno real la separación perfecta y la calibración ideal son inusuales. Conviene repetir este análisis con datos crudos –sin upsampling ni SMOTE– para validar robustez.


---

## 5.5 Evaluación del riesgo de overfitting

Aunque el modelo de regresión logística alcanza una precisión perfecta del 100% tanto en el conjunto de test como en la matriz de confusión, este resultado debe interpretarse con cautela. En contextos reales, una predicción sin errores es extremadamente poco común y puede ser indicativa de **sobreajuste** (*overfitting*), especialmente cuando se ha utilizado *up-sampling*, que consiste en duplicar instancias de la clase minoritaria para equilibrar el dataset.

Para evaluar si el modelo realmente generaliza bien o si está memorizando patrones específicos, se ha aplicado una técnica robusta: la **validación cruzada de 10 particiones (10-fold cross-validation)**. Esta estrategia divide el dataset balanceado (`df_balanced`) en 10 subconjuntos (folds), y entrena y valida el modelo en diferentes combinaciones de estos, minimizando el riesgo de obtener resultados sesgados debido a una división aleatoria favorable.

```{r overfitting-evaluation}
library(caret)

# Control con validación cruzada de 10 particiones
train_control <- trainControl(method = "cv", number = 10)

# Entrenar modelo con CV
model_cv <- train(Target ~ ., data = df_balanced, 
                  method = "glm", family = "binomial",
                  trControl = train_control)

print(model_cv)
```

Los resultados muestran un **Accuracy (media)**: 1.00 y un valor **Kappa**: 1.00.

Estos valores confirman que el modelo mantiene un rendimiento perfecto incluso bajo validación cruzada, lo cual refuerza su consistencia. No obstante, este comportamiento podría deberse a ciertas características del dataset, como una **alta separabilidad entre clases**, un **diseño experimental simulado** o la **limpieza extrema de los datos**.

Aunque los resultados sugieren que el modelo generaliza correctamente en este entorno, no puede descartarse completamente la posibilidad de sobreajuste, ya que el balanceo mediante duplicación podría facilitar que el modelo memorice ejemplos en lugar de aprender patrones generalizables.

Para validar su robustez en escenarios más realistas, se recomienda en futuras iteraciones evaluar el modelo sobre un conjunto externo real no balanceado, comparar con otros algoritmos más complejos (Random Forest, XGBoost) y/o analizar la estabilidad del modelo ante ruido o registros incompletos.

Este enfoque permitirá comprobar su aplicabilidad en entornos industriales reales, donde los datos no siempre cumplen condiciones ideales. Aun así, los resultados actuales constituyen una excelente base inicial para validar la viabilidad de modelos predictivos en mantenimiento industrial.

### 5.5.1 Experimento de “muestra pequeña”

**Objetivo:** Comprobar la sensibilidad del modelo a la cantidad de datos de entrenamiento y observar si se manifiesta sobre‑ajuste cuando se entrena con solo una fracción del dataset.

*Diseño*: se toman 10 réplicas estratificadas que contienen el **25%** del `train_base`. Cada réplica se balancea con *upsampling* (la técnica elegida), se entrena una regresión logística y se evalúa siempre en el **mismo** `test_base` intacto.

---

```{r 5-6-2-setup, message=FALSE}
if (!exists("train_base") | !exists("test_base")) {
  set.seed(123)
  idx_base   <- caret::createDataPartition(df$Target, p = 0.80, list = FALSE)
  train_base <- df[idx_base, ]
  test_base  <- df[-idx_base, ]
}

# ▸Paquetes ------------------------------------------------------------
library(caret)       # createDataPartition, upSample
library(dplyr)
library(purrr)       # map_dfr
library(yardstick)   # metrics
library(pROC)

set.seed(42)
```

```{r 5-6-2-experiment, echo=TRUE}
# Parámetros del experimento
sample_frac <- 0.25   # 25% del train_base
n_reps      <- 10     # número de réplicas

small_metrics <- map_dfr(1:n_reps, function(i) {
  # 1 Sub‑muestra estratificada del train_base
  idx_small   <- createDataPartition(train_base$Target, p = sample_frac, list = FALSE)
  train_small <- train_base[idx_small, ]

  # 2 Balanceo por upsampling (misma técnica oficial)
  train_bal <- upSample(x = train_small[ , -which(names(train_small) == "Target")],
                        y = train_small$Target, yname = "Target")

  # 3 Entrenamiento del modelo
  model_small <- glm(Target ~ ., data = train_bal, family = binomial())

  # 4 Predicción en test_base intacto
  prob <- predict(model_small, newdata = test_base, type = "response")
  pred <- factor(ifelse(prob > 0.5, 1, 0), levels = levels(test_base$Target))

  tibble(replica = i,
         AUC     = pROC::roc(test_base$Target, prob, levels = rev(levels(test_base$Target)))$auc |> as.numeric(),
         BalAcc  = bal_accuracy_vec(test_base$Target, pred),
         F1      = f_meas_vec      (test_base$Target, pred, event_level = "second"))
})

# Resumen estadístico de las réplicas
summary_small <- small_metrics %>%
  summarise(across(AUC:F1, list(mean = ~round(mean(.),3), sd = ~round(sd(.),3))))

knitr::kable(summary_small, caption = "Rendimiento medio (±sd) con solo el 25% del train_base")
```

```{r 5-6-2-plot, echo=FALSE, warning=FALSE}
# Visual opcional de la dispersión de AUC por réplica
library(ggplot2)
small_metrics %>%
  ggplot(aes(x = replica, y = AUC)) +
  geom_point(size = 3) +
  ylim(0.8,1) +
  labs(title = "AUC en cada réplica con muestra pequeña",
       x = "Réplica", y = "AUC")
```

---

#### **Interpretación**

El experimento con el **25% del `train_base`** arroja valores de AUC, Balanced Accuracy y F1 exactamente iguales a 1 en las 10 réplicas (desviación estándar 0). El gráfico confirma que **todas las réplicas** alcanzan AUC = 1.

* **Conclusión inmediata** – El modelo logra una frontera lineal perfecta incluso con un cuarto de los datos. No muestra síntomas de *high variance*; la separación entre clases es tan evidente que el tamaño de muestra no condiciona el rendimiento.

* **Implicación sobre el sobreajuste** – El excelente desempeño no proviene de memorizar ejemplos duplicados (upsampling); se explica por la información altamente discriminante que contienen las variables. El riesgo de *overfitting* clásico (gran varianza al reducir datos) **no se manifiesta**.

* **Limitación** – La prueba se realiza sobre un *test* derivado del mismo dominio y con balanceo artificial. En datos reales no balanceados la AUC podría descender. Aun así, los resultados indican que el modelo es estable frente a variaciones razonables del tamaño de entrenamiento.

**Recomendación**  
> Para verificar robustez en entorno productivo conviene repetir esta prueba con (i) datos de planta sin balancear y (ii) porcentajes de muestra aún menores (10%, 5%) o incluso validación temporal progresiva si existen series de tiempo.

---

## 5.6 Análisis de residuos: homocedasticidad y linealidad

En modelos lineales clásicos, como la regresión lineal, es habitual analizar los residuos frente a los valores ajustados para comprobar si se cumplen ciertos supuestos fundamentales del modelo. Entre ellos, destacan dos:

- **Linealidad**: se refiere a que la relación entre las variables predictoras y la variable dependiente debe ser aproximadamente lineal. En otras palabras, el efecto de cada predictor sobre la variable objetivo debe poder representarse con una línea recta.

- **Homocedasticidad**: significa que la **varianza de los errores (residuos)** debe ser constante a lo largo de todos los niveles de las variables independientes. Si la varianza de los errores cambia (por ejemplo, crece o disminuye con el valor ajustado), se habla de **heterocedasticidad**, lo que puede afectar la validez de las inferencias estadísticas en un modelo lineal.

Sin embargo, en el caso de la **regresión logística**, estos supuestos **no se aplican de forma directa**:

- El modelo estima probabilidades, no valores continuos, por lo que la relación esperada no es lineal entre predictores y la variable dependiente, sino entre los predictores y el **logit** (logaritmo del cociente de probabilidades).

- Los residuos deviance no siguen una distribución continua ni presentan varianza constante, por lo que el supuesto de homocedasticidad **no tiene sentido en este contexto**.

Por estos motivos, se ha decidido no representar el gráfico de residuos frente a valores ajustados, ya que su uso, aunque frecuente en regresión lineal, no es válido ni informativo para evaluar una regresión logística.

Este apartado se mantiene como reflexión crítica, ya que en entregas anteriores se sugirió realizar este tipo de análisis. A partir de la revisión bibliográfica y metodológica, se ha optado por omitirlo conscientemente, sustituyéndolo por herramientas más adecuadas para modelos de clasificación: **validación cruzada**, **métricas de rendimiento** y **análisis de colinealidad**.

## 5.7 Evaluación alternativa con Naïve Bayes

Con el objetivo de comparar la regresión logística con otro enfoque clásico de clasificación, se ha evaluado también un modelo basado en **Naïve Bayes**. Este algoritmo se basa en el **teorema de Bayes** con una fuerte suposición de **independencia condicional entre los predictores**, lo cual rara vez se cumple en la práctica. Aun así, Naïve Bayes suele ofrecer buenos resultados en muchos problemas reales debido a su simplicidad y eficiencia.

Además, Naïve Bayes es un modelo **rápido, simple y eficaz con pocos datos**, lo que lo convierte en una opción interesante en escenarios industriales con recursos computacionales limitados o necesidad de respuesta en tiempo real.

En este experimento, se comparó su rendimiento frente al modelo de regresión logística utilizando el mismo conjunto de datos balanceado y una validación cruzada de 10 particiones (*10-fold cross-validation*), para garantizar una evaluación justa y coherente.

### Entrenamiento del modelo

```{r naive-bayes}
# Definir control de validación cruzada
train_control <- trainControl(method = "cv", number = 10)

# Entrenar modelo Naïve Bayes
model_nb <- train(Target ~ ., data = df_balanced,
                  method = "naive_bayes",
                  trControl = train_control)
print(model_nb)
```

Durante el proceso, se evaluaron dos variantes del modelo: con y sin estimación kernel de las distribuciones. Los valores finales seleccionados automáticamente fueron:

- `usekernel = TRUE`

- `laplace` = 0

- `adjust` = 1

El modelo **sin kernel** obtuvo un rendimiento bajo (67.5% de aciertos), lo que sugiere que **la suposición de independencia condicional y distribución normal de los predictores no se cumple** en este conjunto de datos.

Por el contrario, al activar la opción `usekernel = TRUE`, que permite estimar las distribuciones de forma no paramétrica, el rendimiento mejoró drásticamente, alcanzando una precisión del **99.93%** y un índice Kappa de **0.9986**, muy cercano al rendimiento perfecto de la regresión logística.

Este experimento demuestra que con una mínima adaptación (uso de kernel), **Naïve Bayes puede ser altamente competitivo**, incluso sin necesidad de estructuras complejas ni ajustes exhaustivos. A pesar de su simplicidad, el modelo logra un rendimiento excelente gracias a la calidad del preprocesado y la separabilidad de las clases en el dataset.

---

# 6. Revisión crítica posterior al modelado: impacto de la variable Failure Type

## 6.0 Introducción y agradecimiento

Durante el desarrollo de este trabajo se realizaron diversas pruebas de modelado para predecir el fallo de motores a partir de variables sensoriales. En un principio, los resultados obtenidos por el modelo de regresión logística eran extraordinariamente buenos, alcanzando un AUC de 1, F1 = 1 y clasificación perfecta en todos los experimentos, incluso con subconjuntos reducidos de datos.

Sin embargo, a raíz de un comentario del profesor evaluador, se identificó un posible problema metodológico que podía explicar estos resultados tan inusualmente buenos. En concreto, el profesor sugerió analizar los coeficientes del modelo (valores beta), revisar qué variables aportaban más información, y explorar si se había producido overfitting debido a la presencia de variables excesivamente predictivas o redundantes.

Gracias a este comentario, se descubrió que el dataset contenía una variable denominada `Failure Type` que, de forma directa o indirecta, contenía información equivalente a la variable objetivo `Target`. Este hallazgo dio lugar a una revisión completa del modelo y del pipeline de datos.

Aprovecho esta sección para agradecer sinceramente al profesor su observación, que ha permitido corregir un problema metodológico importante y mejorar sustancialmente el valor formativo y profesional del trabajo.

En los siguientes apartados se detalla cómo se identificó el problema, cuál era su impacto real, cómo se corrigió el dataset, y cuáles son ahora los resultados reales y defendibles del modelo.

## 6.1 Análisis de coeficientes y regularización Lasso

```{r beta-coefficients, echo=TRUE}
# Análisis de los coeficientes del modelo inicial (con 'Failure Type')
summary(model_log$finalModel)
```

Los coeficientes obtenidos muestran valores extremadamente pequeños, errores estándar anormalmente altos y valores p igual a 1. Esto indica que el modelo no está estimando coeficientes significativos. La causa probable es que existe **una separación perfecta en los datos**, es decir, el modelo es capaz de predecir la variable objetivo sin cometer errores, lo que impide una estimación fiable de los parámetros.

Este fenómeno también se confirma al aplicar regularización tipo Lasso:

```{r lasso-regularization, echo=TRUE}
library(glmnet)
x <- model.matrix(Target ~ . -1, data = df_balanced)
y <- as.numeric(as.character(df_balanced$Target))

set.seed(123)
cv_lasso <- cv.glmnet(x, y, family = "binomial", alpha = 1)
plot(cv_lasso)
coef(cv_lasso, s = "lambda.min")
```

El modelo penalizado conserva exclusivamente el término `Failure Type = No Failure`, y elimina todos los demás. Esto confirma que el modelo estaba **apoyándose íntegramente en esta variable para tomar su decisión**, lo cual invalida su interpretabilidad y utilidad real.

Este hallazgo evidencia un caso claro de **data leakage** (fuga de información), ya que `Failure Type` contenía indirectamente la respuesta de la variable objetivo `Target`. La aparición de la etiqueta "No Failure" implica necesariamente `Target = 0`, por lo que el modelo no necesitaba aprender a partir de sensores reales.

En el siguiente apartado se corrige este error y se reentrena el modelo eliminando la variable conflictiva.

## 6.2 Corrección del dataset y reentrenamiento

Para solventar el problema identificado, se realizó una modificación en la fase de preprocesamiento de los datos, eliminando la columna `Failure Type` del conjunto de datos antes de realizar cualquier tipo de balanceo o modelado.

```{r remove-failure-type}
df <- df %>%
  select(-`Failure Type`)  # Eliminamos la columna que generaba data leakage
```

Una vez corregido el dataset, se repitió el proceso completo de:

1. Balanceo del dataset mediante upsampling.

2. División en conjunto de entrenamiento y test.

3. Entrenamiento del modelo de regresión logística.

4. Evaluación del rendimiento del modelo.

```{r re-train-model}
# Balanceo de clases
set.seed(123)
df_upsampled <- upSample(x = df %>% select(-Target), y = df$Target, yname = "Target")

# División en train/test
set.seed(123)
trainIndex <- createDataPartition(df_upsampled$Target, p = .8, list = FALSE)
train <- df_upsampled[trainIndex, ]
test <- df_upsampled[-trainIndex, ]

# Entrenamiento del modelo corregido
model_corr <- train(Target ~ ., data = train, method = "glm", family = "binomial")
```

Este nuevo modelo se ha entrenado únicamente con variables sensoriales reales, sin ninguna información que provenga directa o indirectamente de la variable objetivo. En el siguiente apartado se analizan los resultados obtenidos y se comparan con los anteriores.

## 6.3 Análisis del nuevo modelo

Una vez reentrenado el modelo sin la variable `Failure Type`, los resultados obtenidos reflejan un comportamiento mucho más realista y defendible desde el punto de vista metodológico.

```{r new-model-evaluation}
pred_corr <- predict(model_corr, newdata = test)
confusionMatrix(pred_corr, test$Target)

# Probabilidades para curva ROC y calibración
probs_corr <- predict(model_corr, newdata = test, type = "prob")
roc_corr <- roc(response = test$Target, predictor = probs_corr[,2])
plot(roc_corr, main = "Curva ROC - Modelo corregido")
auc(roc_corr)
```

Las nuevas métricas clave son:

- **Accuracy:** ≈ 0.82

- **F1 Score:** ≈ 0.82

- **Balanced Accuracy:** ≈ 0.82

- **AUC:** ≈ 0.92

Estas cifras indican que el modelo generaliza bien, con un buen equilibrio entre sensibilidad y especificidad, y un excelente poder discriminativo (AUC).

Además, los coeficientes del modelo corregido son ahora interpretables y muestran que las variables `Torque`, `Tool wear` y `Rotational speed` son estadísticamente significativas.

```{r new-model-coefficients}
summary(model_corr$finalModel)
```

El análisis de regularización tipo Lasso también confirma esta interpretación, ya que ahora conserva varias variables sensoriales en lugar de una sola categórica:

```{r lasso-corrected}
x_corr <- model.matrix(Target ~ . -1, data = df_upsampled)
y_corr <- as.numeric(as.character(df_upsampled$Target))

cv_lasso_corr <- cv.glmnet(x_corr, y_corr, family = "binomial", alpha = 1)
plot(cv_lasso_corr)
coef(cv_lasso_corr, s = "lambda.min")
```

Finalmente, la curva de calibración obtenida ahora refleja una predicción más natural, con puntos dispersos y ajustados a la diagonal, lo que indica que el modelo está bien calibrado y no sobreajustado al conjunto de entrenamiento.

En conjunto, estos resultados confirman que la eliminación de `Failure Type` ha permitido construir un modelo más realista, robusto e interpretable.

## 6.4 Reflexión crítica

Este ejercicio ha sido una lección clave sobre la importancia de revisar los resultados con mirada crítica y no dar por válidos los buenos resultados sin analizar su origen. La detección del problema de fuga de información ha puesto de manifiesto un error metodológico relevante, que podría haber invalidado por completo la utilidad del modelo en un entorno real.

Gracias a la revisión inducida por el comentario del profesor, se ha podido corregir este error y construir un modelo más honesto, cuyo rendimiento es muy bueno y está basado únicamente en variables sensoriales reales, replicables en producción.

Esta experiencia también demuestra la importancia de interpretar los coeficientes del modelo y aplicar técnicas de regularización como Lasso no solo para mejorar la precisión, sino también para validar la robustez del aprendizaje.

En el contexto profesional, documentar este tipo de errores no resta valor al trabajo; al contrario, lo engrandece. Refleja capacidad de autocrítica, mejora continua y comprensión profunda del proceso de modelado. Si este proyecto se desplegara en producción, esta revisión habría evitado un modelo inútil o incluso peligroso.

En resumen, esta corrección no solo mejora el trabajo, sino que convierte un fallo en una fortaleza formativa de gran valor.

---

# 7. Conclusiones

### Primera Entrega Intermedia:

El modelo de regresión logística entrenado sobre el conjunto balanceado ha mostrado un rendimiento **perfecto** sobre los datos de test. La precisión global fue del **100%**, sin errores en la predicción de ninguna clase.

La matriz de confusión indica que todos los motores fueron correctamente clasificados como fallidos o no fallidos. Las métricas fueron:

- **Accuracy**: 1.0
- **Precision**: 1.0
- **Recall**: 1.0
- **F1-score**: 1.0

Este comportamiento ideal puede deberse a una combinación de un conjunto de datos bien definido, un modelo simple pero efectivo, y la técnica de *up-sampling* que ayudó a equilibrar correctamente las clases. Aun así, es recomendable evaluar este modelo con nuevas particiones o validación cruzada para asegurar que no hay sobreajuste.

Las variables que más influyeron en las predicciones fueron:
- `Torque [Nm]`
- `Rotational speed [rpm]`
- `Tool wear [min]`

Como mejora futura, se pueden probar otros algoritmos (Random Forest, XGBoost), ajustar umbrales de clasificación para contextos más críticos, o evaluar la robustez del modelo ante datos ruidosos o incompletos.

En conjunto, los resultados muestran que es posible predecir con alta fiabilidad los fallos en motores industriales usando aprendizaje automático supervisado y un buen preprocesado de datos.

En la siguiente fase del proyecto, se pretende extender el análisis hacia la predicción del tipo específico de fallo (`Failure Type`). Para ello, se plantea:
- Tratar `Failure Type` como una variable categórica (`factor`) y aplicar clasificación multiclase.
- Evaluar modelos como **Naive Bayes** para detectar patrones específicos.
- Usar **series temporales** si se dispone de datos secuenciales, lo que permitiría anticipar el fallo.
- Explorar la **teoría de colas** para modelar tiempos de espera y eficiencia del mantenimiento en entornos industriales.

### Segunda Entrega Intermedia:

En esta segunda entrega intermedia se han aplicado mejoras sustanciales al proyecto de predicción de fallos en motores eléctricos, partiendo de las observaciones recibidas en la primera evaluación. En particular, se corrigió el tratamiento de la variable `Type`, que en lugar de ser codificada mediante *one-hot encoding*, se ha tratado como un factor categórico, reduciendo así posibles problemas de colinealidad y mejorando la interpretación del modelo.

Además, se ha incorporado un análisis completo de la **multicolinealidad** utilizando el **Variance Inflation Factor (VIF)**, que ha confirmado que no existen redundancias estadísticas significativas entre los predictores. También se han evaluado los **residuos deviance** del modelo, mostrando una dispersión homogénea alrededor de cero sin patrones visibles, lo que indica que se cumplen razonablemente los supuestos de linealidad y homocedasticidad.

En cuanto al rendimiento del modelo, tanto la evaluación en el conjunto de test como la **validación cruzada de 10 particiones** han mostrado resultados perfectos, con una precisión (accuracy) y una medida de concordancia (Kappa) de 1.00. Si bien esto puede indicar un modelo altamente efectivo, también plantea dudas razonables sobre un posible **sobreajuste**, especialmente debido al uso de *up-sampling*, que puede favorecer la memorización de patrones.

Por tanto, aunque los resultados son técnicamente sólidos y estadísticamente coherentes, se recomienda mantener una visión crítica y ampliar el análisis en futuras fases del proyecto. Entre las posibles líneas de desarrollo se incluyen:

- Evaluar el modelo sobre un conjunto externo real no balanceado.
- Explorar algoritmos más complejos y robustos, como Random Forest o XGBoost.
- Extender el análisis a una clasificación multiclase basada en la variable `Failure Type`.
- Analizar la sensibilidad del modelo ante datos con ruido o registros incompletos.
- Incorporar métricas complementarias como curvas ROC, AUC o precisión-recall.

En conjunto, se concluye que el modelo actual proporciona una **base muy sólida** para la predicción de fallos en entornos industriales simulados, y sienta las bases para su validación y refinamiento en escenarios más realistas y exigentes.

### Tercera Entrega Intermedia:

En esta tercera y última entrega intermedia, se ha reforzado el carácter explicativo del trabajo con el objetivo de guiar al lector a lo largo del análisis, justificando cada decisión tomada. A raíz de los comentarios del profesor, se ha rediseñado completamente el apartado de análisis exploratorio (EDA), especificando qué variables se visualizan, por qué se seleccionan y qué se espera obtener con cada gráfico.

También se ha mejorado la narrativa de las fases de preprocesado, explicando detalladamente la conversión de variables categóricas, la eliminación de redundancias y el uso de técnicas de balanceo como el up-sampling. En particular, se ha aclarado el motivo por el que Target, a pesar de ser binaria, se convierte a factor: esto permite al motor de entrenamiento de caret tratarla correctamente como variable categórica durante la clasificación.

Uno de los puntos clave en esta revisión ha sido el análisis de residuos. En entregas anteriores, se había incluido un gráfico de residuos vs valores ajustados, siguiendo una lógica propia de modelos lineales. En esta versión, se ha decidido no generar dicho gráfico, explicando por qué no es aplicable en regresión logística y reconociendo explícitamente que su inclusión previa fue un error metodológico.

Por recomendación directa del profesor, también se ha incluido un experimento complementario con Naïve Bayes, comparando su rendimiento frente a la regresión logística. Los resultados han mostrado que, si bien la versión clásica del modelo (sin kernel) ofrece un rendimiento limitado, la variante con usekernel = TRUE alcanza una precisión del 99.93%, lo que valida la robustez de los datos y el preprocesado.

Si bien los resultados siguen siendo técnicamente impecables, se mantiene una postura prudente ante la posibilidad de sobreajuste. Como siguiente paso, se propone validar los modelos en condiciones más realistas y extender el análisis hacia tareas más complejas, como la predicción del tipo de fallo (Failure Type).

### Pre Conclusión final

Este proyecto demuestra que, con un preprocesado cuidadoso y un modelo interpretable (regresión logística), es posible predecir con exactitud los fallos de motores eléctricos en un entorno industrial simulado.

**Separabilidad del problema.**
Las variables operativas (torque, velocidad y desgaste) contienen señales suficientemente fuertes como para obtener AUC = 1 incluso con solo un 25 % de los datos de entrenamiento. La frontera lineal es estable y la multicolinealidad es despreciable (VIF ≈ 1).

**Balanceo de clases.**
Se evaluaron upsampling, SMOTE y undersampling. Todas logran métricas perfectas, pero el upsampling aleatorio se adopta por ser el método más sencillo, auditable y sin generación de datos sintéticos.

**Evaluación exhaustiva.**
La combinación de matriz de confusión, curva ROC, curva de calibración y validación cruzada 10-fold confirma la ausencia de sobreajuste dentro del dominio simulado. Las probabilidades están perfectamente calibradas, aunque son extremas (0 o 1), lo que sugiere usar umbrales adaptativos o recalibración con datos reales si se necesita graduar riesgos.

**Prueba de robustez con muestra reducida.**
Diez réplicas con solo el 25 % del train mantuvieron AUC = 1 (sd = 0), indicando que el tamaño de muestra no es un factor crítico mientras la distribución de variables se conserve.

**Comparativa de algoritmos.**
Naïve Bayes con kernel se aproxima al rendimiento de la regresión logística (99.9 % de accuracy) demostrando que el preprocesado es más determinante que la elección del clasificador en este dataset.

## Conclusión Final

El presente trabajo ha tenido como objetivo diseñar un modelo de aprendizaje automático para predecir fallos en motores industriales. A lo largo del proceso se han abordado distintos retos técnicos: desde el análisis exploratorio y el preprocesamiento de datos hasta el diseño, validación y evaluación de modelos de clasificación.

Entre los principales logros se encuentra la correcta integración de técnicas de balanceo, validación cruzada y análisis de residuos. Asimismo, se han aplicado métricas sólidas (F1, AUC, Matriz de Confusión, Curvas ROC) para evaluar el rendimiento.

Lo más relevante ha sido la capacidad para detectar y corregir un fallo metodológico crítico relacionado con fuga de información. Este descubrimiento ha permitido reforzar el aprendizaje práctico sobre la importancia de la integridad de los datos, así como sobre las implicaciones de incluir variables inapropiadas en modelos supervisados.

El modelo final, reentrenado con un conjunto de variables sensoriales limpias, ha demostrado una buena capacidad predictiva, con valores de AUC superiores al 0.9 y una F1 score equilibrada.

Este proyecto deja una base sólida para continuar explorando modelos más avanzados (e.g., Random Forest, XGBoost), incorporar análisis de causa del fallo o desplegar el sistema en un entorno industrial real.



---

# 8. Bibliografía

- Kuhn, M. and Johnson, K. (2013). *Applied Predictive Modeling*. Springer. ISBN 978-1-4614-6848-6.

- James, G., Witten, D., Hastie, T., and Tibshirani, R. (2021). *An Introduction to Statistical Learning with Applications in R*. 2ª edición. Springer.

- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.

- UCI Machine Learning Repository. (2024). *Machine Predictive Maintenance Dataset*. Disponible en: https://archive.ics.uci.edu

- Hastie, T., Tibshirani, R. and Friedman, J. (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*. Springer.

- Breiman, L. (2001). Random Forests. *Machine Learning*, 45(1), pp.5–32.
